---
author: "Anders Torstensson"
date: "`r Sys.Date()`"
params: 
  dataset: "Svea" # "Svea" or "Tangesund"
  year: [2025] # each year separated by comma, e.g. [2022, 2023, 2024]
  remove_flagged_data: !r c("bubbles", "incomplete", "near land") # What flags should be used for filtering data e.g. c("bubbles", "incomplete", "near land")
  class_score_version: 1
  save_psd_plots: FALSE

output: html_document
knit: (function(inputFile, encoding) {
                        rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=paste0("output/html_reports/ifcb_metadata_export_report_",
                        format(Sys.time(), "D%Y%m%dT%H%M"),
                        ".html")) })
title: "`r paste('ifcb metadata-pipeline:', params$dataset, 'dataset')`"
---

# Introduction

This document summarizes the metadata export process from the Imaging FlowCytobot (IFCB) to [IFCB Dashboard](https://ifcb-dashboard-utv.smhi.se/). IFCB images has been classified using an random forest algorithm using the MATLAB package [`ifcb-analysis`](https://github.com/hsosik/ifcb-analysis) (Sosik and Olson 2007). The data pipeline involves several key steps: environment setup, file path definition, IFCB data reading and processing, quality control, and metadata export in .csv files that can be uploaded to IFCB Dashboard. Class score files from the Baltic Sea and Skagerrak-Kattegat are combined in a single folder for integration with Dashboard. This pipeline is facilitated by the R package [`iRfcb`](https://github.com/EuropeanIFCBGroup/iRfcb) (Torstensson 2024). 
 
Parameters are defined to specify the dataset (Tångesund or Svea), year, data removal flags, and data delivery options. 

## Setup

In the setup section, we define essential options and load the required libraries to ensure the environment is correctly configured for the subsequent data processing steps. The [`svepa_event`](https://github.com/nodc-sweden/svepa_event) tool is used to gather information from the event system SVEPA used onboard R/V Svea. Additionally, we configure the necessary paths and virtual environments. If a detailed coastline shapefile does not already exist, one is downloaded from [EEA](https://www.eea.europa.eu/ds_resolveuid/3e10d9c027734ae2ae5f95309a78c259).

```{r setup, include=FALSE}
# Setting options for 'knitr' package chunks to display code in the output
knitr::opts_chunk$set(echo = TRUE)

# Start time
knit.time <- Sys.time()

# Load required libraries
library(iRfcb)
library(tidyverse)
library(worrms)
library(leaflet)
library(htmltools)
library(knitr)
library(patchwork)
library(reticulate)

# Installing the required Python dependencies in a specified virtual environment for the 'iRfcb' package
ifcb_py_install(envname = ".virtualenvs/iRfcb")

# Install svepa_event from Github to access Svea cruise numbers
py_install("git+https://github.com/nodc-sweden/svepa_event.git", pip = TRUE)

# Import the svepa_event Python module
svepa_event <- import("svepa_event")

# Make sure SVEPA is up to date
svepa_event$update_local_svepa_data()

# Wrap svepa_event to catch errors
py_run_string("
def safe_get_svepa_event(svepa_event, platform, timestamp):
    try:
        result = svepa_event.get_svepa_event(platform, timestamp)
        return {'id': result.id if hasattr(result, 'id') else None, 'error': None}
    except AttributeError as e:
        return {'id': None, 'error': str(e)}
    except Exception as e:
        return {'id': None, 'error': str(e)}
")

# Define paths from .Renviron, perhaps not necessary
tlc_path <- Sys.getenv("tlc_path")
tk_path <- Sys.getenv("tk_path")

# Set TCL and TK PATH for knit, perhaps not necessary
Sys.setenv(TCL_LIBRARY = tlc_path)
Sys.setenv(TK_LIBRARY = tk_path)

# Define the URL of a EEA shapefile and the target directory
url <- "https://www.eea.europa.eu/data-and-maps/data/eea-coastline-for-analysis-2/gis-data/eea-coastline-polygon/at_download/file"
zip_file <- "data/shapefiles/EEA_Coastline_Polygon_Shape.zip"
extracted_dir <- "data/shapefiles/EEA_Coastline_Polygon_Shape"
shapefile_path <- file.path(extracted_dir, "EEA_Coastline_20170228.shp")

# Check if the shapefile already exists
if (!file.exists(shapefile_path)) {
  
  # Create the target directory if it doesn't exist
  if (!dir.exists(extracted_dir)) {
    dir.create(extracted_dir, recursive = TRUE)
  }
  
  # Download the ZIP file
  download.file(url, zip_file, mode = "wb")
  
  # Extract the ZIP file
  unzip(zip_file, exdir = extracted_dir)
  
  # Optionally, remove the downloaded ZIP file after extraction
  file.remove(zip_file)
  
  message("File downloaded and extracted.")
} else {
  message("Shapefile already exists. No download needed.")
}
```

## Data Export Paths

Here, we define the file paths needed for exporting IFCB data. These paths include directories for storing features, HDR data, and MATLAB classified data for each specified year. The code also identifies the correct subfolders for classified data based on version numbers.

```{r define_paths}
# Get data paths to ifcb and ferrybox data folders defined in the projects .Renviron
# Edit the .Renviron by usethis::edit_r_environ("project")
ifcb_path <- Sys.getenv("ifcb_path")
ferrybox_path <- Sys.getenv("ferrybox_path")
havgem_path <- Sys.getenv("havgem_path")

# Initialize list to store paths for each year
feature_folders <- list()
data_folders <- list()
class_folders_sk <- list()
class_folders_baltic <- list()

# Loop through each year in params$year
for (year in params$year) {
  
  # Define paths for feature and HDR data
  feature_folder <- file.path(ifcb_path, "features", year)
  data_folder <- file.path(ifcb_path, "data", year)

  # Append to lists
  feature_folders <- c(feature_folders, feature_folder)
  data_folders <- c(data_folders, data_folder)

  # Define path for the generic class folders
  class_folder_sk <- file.path(ifcb_path, "classified", "Skagerrak-Kattegat")
  class_folder_baltic <- file.path(ifcb_path, "classified", "Baltic")
  
  # List all class versions
  class_subfolders_sk <- list.dirs(class_folder_sk, full.names = TRUE, recursive = FALSE)

  # Find all subfolders from the selected year
  class_subfolders_sk <- class_subfolders_sk[grepl(as.character(year), class_subfolders_sk)]

  # Extract the version numbers from the subfolder names
  versions_sk <- sub(paste0("class", year , "_", "Skagerrak-Kattegat", "_v"), "", basename(class_subfolders_sk))
  versions_sk <- as.numeric(versions_sk)

  # Get the subfolder with the highest version number
  max_version_sk <- max(versions_sk, na.rm = TRUE)
  class_folder_sk <- class_subfolders_sk[which(versions_sk == max_version_sk)]

   # List all class versions
  class_subfolders_baltic <- list.dirs(class_folder_baltic, full.names = TRUE, recursive = FALSE)

  # Find all subfolders from the selected year
  class_subfolders_baltic <- class_subfolders_baltic[grepl(as.character(year), class_subfolders_baltic)]

  # Extract the version numbers from the subfolder names
  versions_baltic <- sub(paste0("class", year , "_", "Baltic", "_v"), "", basename(class_subfolders_baltic))
  versions_baltic <- as.numeric(versions_baltic)

  # Get the subfolder with the highest version number
  max_version_baltic <- max(versions_baltic, na.rm = TRUE)
  class_folder_baltic <- class_subfolders_baltic[which(versions_baltic == max_version_baltic)]

  # Append to list
  class_folders_sk <- c(class_folders_sk, class_folder_sk)
  class_folders_baltic <- c(class_folders_baltic, class_folder_baltic)
}

# Define results folders
output_folder <- file.path(ifcb_path, "shark")
dashboard_metadata_folder <- file.path(ifcb_path, "ifcbdb_metadata")

# Read sample blacklist
blacklist <- read_tsv("data/sample_blacklist.tsv", col_types = cols())

# Create a regex pattern by joining all blacklist samples with `|`
blacklist_pattern <- paste(blacklist$sample, collapse = "|")
```


## Reading HDR Data

In this step, we read the HDR data files for each year specified in the parameters. The HDR data includes sample metadata such as GPS coordinates, timestamps, and related information. We handle any missing GPS data by filling in the gaps using ferrybox positions.

```{r read_hdr_data}
# Initialize a list to store the processed HDR data for each year
hdr_data_list <- list()
all_hdr_data_list <- list()
ferrbyox_data_list <- list()

# Start time
start.time <- Sys.time()

for (i in seq_along(params$year)) {

  # Get current data folder
  data_folder <- data_folders[[i]]

  # Read HDR data from the specified data folder
  all_hdr_data <- ifcb_read_hdr_data(data_folder, gps_only = FALSE, verbose = FALSE)

  if (params$dataset == "Tangesund") {
    
    # Keep only relevant information and add position to Tangesund
    hdr_data <- all_hdr_data %>%
      select(sample, timestamp, date, year, month, day, time, ifcb_number) %>%
      mutate(gpsLatitude = 58.07500,
             gpsLongitude = 11.49300)
  } else {
   
    # Keep only relevant information and NA coordinate if fix is older than 10 minutes
    hdr_data <- all_hdr_data %>%
      select(sample, gpsLatitude, gpsLongitude, timestamp, date, year, month, day, time, ifcb_number, gpsTimeFromFix) %>%
      mutate(gpsTimeFromFix = as.POSIXct(gpsTimeFromFix, format = "%b/%d/%Y %H:%M:%OS", tz = "UTC")) %>%
      mutate(gpsLatitude = ifelse(abs(difftime(gpsTimeFromFix, timestamp, units = "mins")) > 10, NA, gpsLatitude),
             gpsLongitude = ifelse(abs(difftime(gpsTimeFromFix, timestamp, units = "mins")) > 10, NA, gpsLongitude)
      ) %>%
      select(-gpsTimeFromFix)
  }

  # Identify rows in hdr_data where the year is not 2016 and GPS latitude or longitude is missing
  missing_position <- hdr_data %>%
    filter(is.na(gpsLatitude)) %>%
    filter(is.na(gpsLongitude))

  # If there are rows with missing GPS positions
  if (nrow(missing_position) > 0) {
    
    # Retrieve ferrybox positions for the timestamps of the missing GPS data
    ferrybox_positions <- ifcb_get_ferrybox_data(missing_position$timestamp, ferrybox_path)
    
    # Rename GPS latitude and longitude columns in ferrybox_positions to avoid conflicts
    ferrybox_positions <- ferrybox_positions %>%
      rename(gpsLatitude_fb = gpsLatitude,
             gpsLongitude_fb = gpsLongitude)

    # Merge hdr_data with ferrybox_positions based on timestamps
    hdr_data <- hdr_data %>%
      left_join(ferrybox_positions, by = "timestamp") %>%
      mutate(gpsLatitude = coalesce(gpsLatitude, gpsLatitude_fb),
             gpsLongitude = coalesce(gpsLongitude, gpsLongitude_fb)) %>%
      select(-gpsLongitude_fb, -gpsLatitude_fb)
  }
  
  # Define ferrybox parameter numbers to be extracted
  ferrybox_parameters <- c("70", "80070", "8063", "88063", "8165", "88165",
                           "8173", "88173", "8166","88166", "8172", "88172", "8174",
                           "88174", "8177", "88177", "8179", "88179",
                           "8181", "88181", "8190", "88190", "8191", "88191")
  
  if(!params$dataset == "Tangesund") {
    
    # Get ferrybox data for Svea data
    ferrybox_data <- ifcb_get_ferrybox_data(hdr_data$timestamp, 
                                            ferrybox_path, 
                                            parameters = ferrybox_parameters)
  } else {
    
    column_names <- c("timestamp", ferrybox_parameters)
    
    # Create a data frame with one row filled with NA values for Tangesund data
    ferrybox_data <- data.frame(matrix(NA, ncol = length(column_names), nrow = 1))
    colnames(ferrybox_data) <- column_names
  }
  
  # Store the processed hdr_data in the list
  hdr_data_list[[as.character(params$year[i])]] <- hdr_data
  
  # Store all the processed hdr_data in the list
  all_hdr_data_list[[as.character(params$year[i])]] <- all_hdr_data
  
  # Store all the ferrybox data in the list
  ferrbyox_data_list[[as.character(params$year[i])]] <- ferrybox_data
}

# End time
end.time <- Sys.time()
runtime_hdr <- round(end.time - start.time, 2)
```

## Get RV SVEA cruise number from SVEPA
Here sample time stamps are matched with SVEPA using the [`svepa_event`](https://github.com/nodc-sweden/svepa_event) tool to extract cruise numbers for R/V Svea data.

```{r get_svepa, include=TRUE, results = 'hide', echo=TRUE, warning=FALSE}
# Start time
start.time <- Sys.time()

# Read list with additional cruise numbers
additional_cruises <- read_tsv("data/cruise_numbers.txt", col_types = cols()) %>%
  mutate(
    startdate = as.POSIXct(startdate, format = "%Y-%m-%d %H:%M", tz = "UTC"),
    stopdate = as.POSIXct(stopdate, format = "%Y-%m-%d %H:%M", tz = "UTC")
  )

# Initialize a list to store the processed HDR data for each year
svepa_list <- list()

for (i in seq_along(params$year)) {
  
  if (params$dataset == "Tangesund") {
    
    # Create an empty vector
    cruise_numbers <- data.frame(sample = as.character(), cruise_number = as.character())
    
  } else {
    
    # Get current HDR data
    hdr_data <- hdr_data_list[[as.character(params$year[i])]]

    # Create an empty vector
    cruise_numbers <- data.frame(sample = as.character(), cruise_number = as.character())
    
    for (sample in seq_along(hdr_data$sample)) {
      timestamp <- format(hdr_data$timestamp[sample], "%Y%m%d%H%M%S")
      
      # Call the Python function
      svepa_result <- py$safe_get_svepa_event(svepa_event, "SVEA", timestamp)
      
      if (is.null(svepa_result$id)) {
        # Convert timestamp to Date object
        timestamp_datetime <- as.POSIXct(timestamp, format = "%Y%m%d%H%M%S", tz = "UTC")
        
        # Find the cruise number from additional resource
        svepa_id <- additional_cruises %>%
          filter(timestamp_datetime >= startdate & timestamp_datetime <= stopdate) %>%
          pull(cruise_no)
        
        if (length(svepa_id) == 0) {
          svepa_id <- NA
        }
        
      } else {
        svepa_id <- svepa_result$id
      }
      
      cruise_number <- data.frame(sample = sapply(strsplit(hdr_data$sample[sample], "_"), `[`, 1),
                                  cruise_number = svepa_id)
      
      # Add to cruise_numbers df
      cruise_numbers <- bind_rows(cruise_numbers, cruise_number)
    }
  }
  
  # Store the analysis date in the list
  svepa_list[[as.character(params$year[i])]] <- cruise_numbers 
}

# End time
end.time <- Sys.time()
runtime_svepa <- round(end.time - start.time, 2)
```

## Get Tångesund sample depth
Sample depth information from the Tångesund study is read from a .csv file.

```{r get_sample_depth, echo=TRUE}
sample_depth <- read_csv("data/tangesund_depthdata.csv", col_types = cols()) %>%
  distinct()
```

## Quality Control: Particle Size Distribution (PSD)
In the PSD quality control step, we calculate the particle size distribution for each year's data according to Hayashi et al., in prep. This involves determining various parameters such as bead size, bubble presence, and biomass, ensuring that the data meets quality standards.

```{r psd_check, include=TRUE, results = 'hide', echo=TRUE, message=FALSE}
# Start time
start.time <- Sys.time()

# Initialize a list to store PSD results for each year
psd_list <- list()

for (i in seq_along(params$year)) {

  # Get current data folder
  feature_folder <- feature_folders[[i]]
  data_folder <- data_folders[[i]]

  # Define result and plot folder paths
  psd_result_files <- file.path(ifcb_path, "psd", paste0("psd_", params$year[i]))
  psd_plot_folder <- file.path(ifcb_path, "psd", "figures", params$year[i])

  if (params$save_psd_plots) {
    psd_plot_folder <- psd_plot_folder
    
    if (dir.exists(psd_plot_folder)) {
      unlink(psd_plot_folder, recursive = TRUE)
    }
  } else {
    psd_plot_folder <- NULL
  }
  
  # Calculate the particle size distribution (PSD) using IFCB data from the specified folders
  psd <- ifcb_psd(feature_folder = feature_folder,
                  hdr_folder = data_folder,
                  save_data = TRUE,
                  output_file = psd_result_files,
                  plot_folder = psd_plot_folder,
                  use_marker = FALSE,
                  start_fit = 15,
                  r_sqr = 0.5,
                  beads = 10 ** 20,
                  bubbles = 110,
                  incomplete = c(1500, 3),
                  missing_cells = 0.7,
                  biomass = 1000,
                  bloom = 10,
                  humidity = 75,
                  micron_factor = 1/3.4)

  # Store PSD results in the list
  psd_list[[as.character(params$year[i])]] <- psd
}

# End time
end.time <- Sys.time()
runtime_psd <- round(end.time - start.time, 2)
runtime_knit <- round(end.time - knit.time, 2)
```

## Displaying PSD Summary
This section prints a summary of the PSD results for each year. The summary includes a table that groups the quality flags (Q-flags) by their type and shows the number of samples associated with each flag.

```{r psd_print, include=TRUE, results='asis'}
for (i in seq_along(params$year)) {
  # Print PSD summary
  print(psd_list[[as.character(params$year[i])]]$flags %>%
          group_by(flag) %>%
          summarise("Number of samples" = n()) %>%
          arrange(desc(`Number of samples`)) %>%
          rename("Q-flag" = flag) %>%
          knitr::kable(caption = paste0("PSD Summary for Year ",
                                       params$year[i],
                                       " (total n samples: ",
                                       nrow(psd_list[[as.character(params$year[i])]]$fits),
                                       ")")))
}
```

## Coordinate Checks
We perform a geographic check on the samples' coordinates to determine whether they are near land (i.e. in harbor) or within the Baltic basin. This check ensures the spatial accuracy of the collected data.

```{r coordinate_check}
# Initialize a list to store coordinate check results for each year
coordinate_check_list <- list()

for (i in seq_along(params$year)) {

  # Get current HDR data
  hdr_data <- hdr_data_list[[as.character(params$year[i])]]

  # Create a data frame with sample names and GPS coordinates from hdr_data
  positions <- data.frame(
    sample = sapply(strsplit(hdr_data$sample, "_"), `[`, 1),
    gpsLatitude = hdr_data$gpsLatitude,
    gpsLongitude = hdr_data$gpsLongitude) %>%
    filter(!is.na(gpsLatitude) | !is.na(gpsLongitude))

  # Determine if positions are near land using specified shapefile
  near_land <- ifcb_is_near_land(
    positions$gpsLatitude,
    positions$gpsLongitude,
    distance = 500, # Släggö is about 600 m from Land
    shape = shapefile_path)

  # Determine if positions are in the Baltic basin
  in_baltic <- ifcb_is_in_basin(
    positions$gpsLatitude,
    positions$gpsLongitude)

  # Add the near_land and in_baltic information to the positions data frame
  positions$near_land <- near_land
  positions$in_baltic <- in_baltic
  positions$basin <- ifcb_which_basin(positions$gpsLatitude, positions$gpsLongitude)

  # Store coordinate check results in the list
  coordinate_check_list[[as.character(params$year[i])]] <- positions
}
```

## Combining Q-Flags with GPS Coordinates
In this step, we combine the PSD quality flags with the GPS coordinates. We then categorize the flags and map the samples using different markers based on their quality.

```{r q_flags, message=FALSE}
# Initialize lists to store results for each year
qflags_list <- list()
date_info_list <- list()

for (i in seq_along(params$year)) {

  # Get current PSD flags and positions data
  psd_flags <- psd_list[[as.character(params$year[i])]]$flags
  positions <- coordinate_check_list[[as.character(params$year[i])]]

  # Join psd$flags with positions data by "sample", add near_land_qflag, unite into flag, convert to sentence case
  qflags <- psd_flags %>%
    full_join(positions, by = "sample") %>%
    mutate(near_land_qflag = ifelse(near_land, "Near land", NA)) %>%
    unite(col = flag, flag, near_land_qflag, na.rm = TRUE, sep = ", ") %>%
    mutate(flag = ifelse(str_to_sentence(flag) == "", NA, str_to_sentence(flag)),
           lon = gpsLongitude,
           lat = gpsLatitude) %>%
    select(sample, flag, lat, lon) %>%
    mutate(group = ifelse(is.na(flag), "blue", "red"))

  # Convert filenames in biovolume_data$sample to date information
  date_info <- ifcb_convert_filenames(qflags$sample)

  # Join qflags with date_info by "sample"
  qflags <- qflags %>%
    left_join(date_info, by = "sample")

  # Store qflags results in the list
  qflags_list[[as.character(params$year[i])]] <- qflags

  # Store date info in the list
  date_info_list[[as.character(params$year[i])]] <- date_info
}
```

## Visualizing QC Maps
The QC maps visually display the samples' locations on a map, using color-coded markers to represent different quality flags. The maps are organized by month, providing an intuitive overview of the spatial distribution of the samples.

```{r qc_maps, results='asis'}
# Initialize a list to store all qc_maps HTML
qc_maps_list <- list()

for (i in seq_along(params$year)) {

  # Get current Q-flags data for the year
  qflags <- qflags_list[[as.character(params$year[i])]]

  # URL to leaflet icons
  blue_marker <- "https://raw.githubusercontent.com/pointhi/leaflet-color-markers/master/img/marker-icon-blue.png"
  red_marker <- "https://raw.githubusercontent.com/pointhi/leaflet-color-markers/master/img/marker-icon-red.png"
  
  # Define the icon list with URLs for the markers
  sampleIcons <- iconList(
    blue = makeIcon(iconUrl = blue_marker, iconWidth = 24, iconHeight = 32),
    red = makeIcon(iconUrl = red_marker, iconWidth = 24, iconHeight = 32)
  )

  # Create maps for each month and display them
  month_names <- c("January", "February", "March", "April", 
                   "May", "June", "July", "August", "September", 
                   "October", "November", "December")

  for (j in 1:12) {
    gps_month <- qflags %>% filter(month(date) == j)

    if (nrow(gps_month) > 0) {
      map <- leaflet(data = gps_month) %>%
        addTiles() %>%
        addMarkers(
          lng = ~lon,
          lat = ~lat,
          icon = ~sampleIcons[group],
          popup = ~ifelse(is.na(flag), 
                          paste("Sample:", sample), 
                          paste("Sample:", sample, 
                                "<br>", 
                                "QFlag:", flag))
        )
      qc_maps_list <- c(qc_maps_list, paste(month_names[j], params$year[i]), list(map))
    }
  }
}
```

## Rendering QC Maps
This section renders the QC maps generated in the previous step, displaying them in the output document. All data that has coordinates are displayed here; data with Q-flags defined in params$remove_flagged_data will be removed in a later step.

```{r render_qc_maps, include=TRUE}
tagList(qc_maps_list)
```

## Merging Data
In the final data processing step, we merge the biovolume data, HDR data, ferrybox data, qflags, cruise numbers and coordinates etc into yearly data frames. 

```{r merge_data, message=FALSE}
# Initialize a list to store aggregated data frames for each year
metadata_aggregated_list <- list()

for (i in seq_along(params$year)) {

  # Extract relevant data frames for the current year
  date_info <- hdr_data_list[[as.character(params$year[i])]] %>%
    mutate(pid = sample,
           sample = sapply(strsplit(sample, "_"), `[`, 1)) %>%
    select(-gpsLatitude, -gpsLongitude)
  qflags_select <- select(qflags_list[[as.character(params$year[i])]], sample, flag)
  positions <- coordinate_check_list[[as.character(params$year[i])]]
  ferrybox_data <- ferrbyox_data_list[[as.character(params$year[i])]]
  cruise_numbers <- svepa_list[[as.character(params$year[i])]]
  bad_samples <- blacklist %>%
    filter(grepl(paste0("^D", params$year[i]), sample)) %>%
    select(sample) %>%
    rename(pid = sample) %>%
    mutate(qc_bad = TRUE,
           skip = TRUE)
    
  # Join metadata
  metadata <- date_info %>%
    left_join(qflags_select, by = "sample") %>%
    left_join(positions, by = "sample") %>%
    left_join(cruise_numbers, by = "sample") 
    
  # Compile Dashboard Metadata
  metadata_db <- metadata %>%
    select(pid, ifcb_number, year, gpsLatitude, gpsLongitude, flag, cruise_number) %>%
    distinct() %>%
    mutate(latitude = gpsLatitude,
           longitude = gpsLongitude,
           depth = ifelse(params$dataset == "Tangesund", "", 4),
           qc_bad = grepl(paste(params$remove_flagged_data, collapse = "|"), flag, ignore.case = TRUE),
           skip = str_detect(pid, blacklist_pattern),
           sample_type = ifelse(params$dataset == "Tangesund", "cast", "underway"),
           cruise = paste0("SVEA_", year, "_", cruise_number)) %>%
    select(-gpsLatitude, -cruise_number, -gpsLongitude, -flag, -year, -ifcb_number) %>%
    bind_rows(bad_samples)
  
  # Compile metadata for taxonomist
  metadata_annotation <- metadata %>%
    select(pid, in_baltic, basin, near_land, flag, cruise_number)
  
  # Use actual depths for Tångesund and remove cruise numbers
  if (params$dataset == "Tangesund") {
    metadata_db <- metadata_db %>%
      select(-depth) %>%
      left_join(sample_depth, by = c("pid" = "sample")) %>%
      mutate(cruise = NA)
  }
  
  # Create directories if they do not exist
  if (!dir.exists(dashboard_metadata_folder)) {
    dir.create(dashboard_metadata_folder, recursive = TRUE)
  }

  # Define file path for the output CSV
  csv_file_path <- file.path(dashboard_metadata_folder, paste0("ifcb_dashboard_metadata_", params$dataset, "_", params$year[i], ".csv"))
  
  # Write Dashboard metadata
  write_csv(metadata_db,
            csv_file_path,
            na = "")
  
    
  out_path <- file.path(havgem_path, "annotation", Sys.Date())
  
  if (!dir.exists(out_path)) {
    dir.create(out_path, recursive = TRUE)
  }
  
  # Define file path for the output CSV
  txt_file_path <- file.path(out_path, paste0("ifcb_sample_metadata_", params$dataset, "_", params$year[i], ".txt"))
  
  # Write Dashboard metadata
  write_tsv(metadata_annotation,
            txt_file_path,
            na = "")
  
  # Output message showing the file path of the saved CSV
  cat("Dashboard metadata saved to:", csv_file_path, "\n")
  
  # Store aggregated data for the current year in the list
  metadata_aggregated_list[[as.character(params$year[i])]] <- metadata
}
```

## Copy Classified .mat Files

Here, we merge classified files from the Baltic Sea and Skagerrak-Kattegat into a single folder that can be used for the R/V Svea dataset in Dashboard.

```{r copy_mat_files}
if (!params$dataset == "Tangesund") {
  for (i in seq_along(params$year)) {
    
    # Extract metadata for the current year from a pre-aggregated list of metadata
    metadata <- metadata_aggregated_list[[as.character(params$year[i])]]
    
    # Filter samples in the Baltic Sea
    baltic_samples <- metadata %>%
      filter(in_baltic)
    
    # Filter samples outside the Baltic Sea (e.g., Skagerrak-Kattegat)
    sk_samples <- metadata %>%
      filter(!in_baltic)
    
    # Retrieve class files for Skagerrak-Kattegat for the current iteration
    sk_class_files <- data.frame(files = list.files(class_folders_sk[[i]], 
                                                    pattern = ".mat", 
                                                    full.names = TRUE)) %>%
      mutate(pid = gsub(paste0("_class_v", params$class_score_version, ".mat"), "", basename(files)))
    
    # Retrieve class files for Baltic for the current iteration
    baltic_class_files <- data.frame(files = list.files(class_folders_baltic[[i]], 
                                                        pattern = ".mat",
                                                        full.names = TRUE)) %>%
      mutate(pid = gsub(paste0("_class_v", params$class_score_version, ".mat"), "", basename(files)))
    
    # Filter Skagerrak-Kattegat class files to include only those matching pids in sk_samples
    sk_files_to_copy <- sk_class_files %>%
      filter(pid %in% sk_samples$pid)
    
    # Filter Baltic class files to include only those matching pids in baltic_samples
    baltic_files_to_copy <- baltic_class_files %>%
      filter(pid %in% baltic_samples$pid)
    
    # Define the path for the combined classification folder for the specific year and class score version
    class_folder <- file.path(ifcb_path, 
                              "classified", 
                              "Baltic-Skagerrak-Kattegat-dashboard", 
                              paste0("class", params$year[i], "_v", params$class_score_version))
    
    # Create the directory if it doesn’t exist, allowing for nested folder creation
    if (!dir.exists(class_folder)) {
      dir.create(class_folder, recursive = TRUE)
    }
    
    # Copy selected Skagerrak-Kattegat files to the combined classification folder
    copy_sk <- file.copy(sk_files_to_copy$files,
                         class_folder,
                         overwrite = TRUE)
    
    cat("Copied", sum(copy_sk), "Skagerrak-Kattegat files to folder", class_folder, "\n")

    
    # Copy selected Baltic files to the combined classification folder
    copy_baltic <- file.copy(baltic_files_to_copy$files,
                             class_folder,
                             overwrite = TRUE)
    
    cat("Copied", sum(copy_baltic), "Baltic files to folder", class_folder, "\n")
  }
}
```

## Count annotations

In this step, we count the number of annotated images for each classifier and exports data into a .txt file, facilitating image annotation.

```{r count_mat_annotation}
# Start time
start.time <- Sys.time()

# Define the different classifiers
classifiers <- c("Baltic", "Skagerrak-Kattegat", "smhi_niva")

# Count the manual annotations and save as .txt
for (classifier in classifiers) {
  class_counts <- ifcb_count_mat_annotations(file.path(ifcb_path, "manual", classifier),
                                             file.path(ifcb_path, "config", paste0("class2use_", classifier, ".mat")),
                                             skip_class = 1)
  
  class_counts <- arrange(class_counts, desc(n))
  
  out_path <- file.path(havgem_path, "annotation", Sys.Date())
  
  if (!dir.exists(out_path)) {
    dir.create(out_path, recursive = TRUE)
  }
  
  write_tsv(class_counts,
            file.path(out_path, paste0("class_counts_", classifier, "_", Sys.Date(), ".txt")),
            na = "")
  
}

# End time
end.time <- Sys.time()
runtime_mat_count <- round(end.time - start.time, 2)
```

## Summarize Runtimes
This section provides a summary of the time taken to run various parts of the script, such as extracting HDR data, analyzing particle size distribution, and generating the entire report. This helps in identifying the computational efficiency of the pipeline.

```{r runtime_summary}
runtime_variables <- c("running the whole script",
                       "extracting HDR data",
                       "extracting cruise numbers",
                       "analysing PSD",
                       "counting manual annotations")

runtime_values <- c(runtime_knit, runtime_hdr, runtime_svepa, runtime_psd, runtime_mat_count)

for (i in seq_along(runtime_variables)) {
  cat("Time taken for ", runtime_variables[i], ": ", round(runtime_values[i]/3600, 2), "h", "\n", sep = "")
}
```

## Reproducibility
To ensure that the results can be reproduced in the future, this section records the session information, including the date and time when the script was run and details about the R environment used. This information is crucial for validating and reproducing the analysis.

```{r reproducibility}
# Date time
cat("Time started:", format(knit.time), "\n")
cat("Time finished:", format(Sys.time()), "\n")

# Here we store the session info for this script
sessioninfo::session_info()
```

## References
- Hayashi, K., Walton, J., Lie, A., Smith, J. and Kudela M. Using particle size distribution (PSD) to automate imaging flow cytobot (IFCB) data quality in coastal California, USA. In prep.
- Sosik, H. M. and Olson, R. J. (2007), Automated taxonomic classification of phytoplankton sampled with imaging-in-flow cytometry. Limnol. Oceanogr: Methods 5, 204-216.
- Torstensson, Anders; Skjevik, Ann-Turi; Mohlin, Malin; Karlberg, Maria; Karlson, Bengt (2024). SMHI IFCB plankton image reference library. SciLifeLab. Dataset. doi:10.17044/scilifelab.25883455

```{r citation, echo=FALSE}
citation(package = "iRfcb")
```
